{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10997607,"sourceType":"datasetVersion","datasetId":6701960},{"sourceId":11121300,"sourceType":"datasetVersion","datasetId":6829363},{"sourceId":11460662,"sourceType":"datasetVersion","datasetId":7143459}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Tracking With Trained ReID","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport cv2\nimport torch\nimport random\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import defaultdict\nimport os\nimport re\nimport cv2\nimport torch\nimport random\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import defaultdict\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\n\n\ndef load_lazy_dataset_from_directory(base_dir, transform=None, object_transform=None, seq_length=5):\n\n    video_paths = []\n    annotation_paths = []\n    lazy_datasets = []\n    \n    frames_dir = os.path.join(base_dir, \"Frames\")\n    labels_dir = os.path.join(base_dir, \"MOT labels\")\n    \n    if not os.path.exists(frames_dir) or not os.path.exists(labels_dir):\n        raise FileNotFoundError(f\"Frames or labels directory does not exist at {base_dir}\")\n    \n                    if os.path.isdir(os.path.join(frames_dir, folder))]\n    \n    for video_folder in video_folders:\n        video_frames_path = os.path.join(frames_dir, video_folder)\n        gt_file_path = os.path.join(labels_dir, video_folder, \"gt\", \"gt.txt\")\n        \n        if not os.path.exists(gt_file_path):\n            print(f\"Warning: No annotations found for {video_folder}, skipping\")\n            continue\n        \n        video_paths.append(video_frames_path)\n        annotation_paths.append(gt_file_path)\n        \n        frames_loader = LazyFrameLoader(video_frames_path)\n        annotations_loader = LazyAnnotationLoader(gt_file_path)\n        \n        video_dataset = LazyTrackingDataset(\n            frames_loader=frames_loader,\n            annotations_loader=annotations_loader,\n            transform=transform,\n            object_transform=object_transform,\n            seq_length=seq_length\n        )\n        \n        lazy_datasets.append(video_dataset)\n    \n    return lazy_datasets, video_paths, annotation_paths","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:22:59.191639Z","iopub.execute_input":"2025-06-29T02:22:59.192281Z","iopub.status.idle":"2025-06-29T02:23:09.234624Z","shell.execute_reply.started":"2025-06-29T02:22:59.192227Z","shell.execute_reply":"2025-06-29T02:23:09.234020Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LazyFrameLoader:\n    def __init__(self, video_path):\n        self.video_path = video_path\n        self.frame_files = sorted(os.listdir(video_path))\n        self.frame_id_map = {}\n        \n        for file in self.frame_files:\n            match = re.search(r'\\d+', file)\n            if match:\n                frame_id = int(match.group())\n                self.frame_id_map[frame_id] = file\n    \n    def __getitem__(self, frame_id):\n        if frame_id not in self.frame_id_map:\n            raise KeyError(f\"Frame ID {frame_id} not found in dataset\")\n            \n        file = self.frame_id_map[frame_id]\n        img_path = os.path.join(self.video_path, file)\n        img = cv2.imread(img_path)\n        \n        if img is None:\n            raise IOError(f\"Failed to load image {img_path}\")\n            \n        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    def keys(self):\n        return list(self.frame_id_map.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:23:09.235935Z","iopub.execute_input":"2025-06-29T02:23:09.236349Z","iopub.status.idle":"2025-06-29T02:23:09.242334Z","shell.execute_reply.started":"2025-06-29T02:23:09.236319Z","shell.execute_reply":"2025-06-29T02:23:09.241642Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class LazyAnnotationLoader:\n    def __init__(self, gt_path):\n        self.gt_path = gt_path\n        self.annotations = None    \n    def _load_annotations(self):\n\n        annotations = defaultdict(list)\n        \n        class_mapping = {\n            \"drowning\": 1,\n            \"not drowning\": 2\n        }\n\n        with open(self.gt_path, \"r\") as f:\n            for line in f:\n                parts = line.strip().split(\",\")\n                \n                if len(parts) >= 9: \n                    frame_id, obj_id, x, y, w, h, conf, cls_raw, vis = parts[:9]\n                    \n                    frame_id = int(float(frame_id))\n                    obj_id = int(float(obj_id))\n                    x, y, w, h = float(x), float(y), float(w), float(h)\n                    conf = float(conf)\n                    vis = float(vis)\n                    \n                    try:\n                        cls = int(float(cls_raw))\n                    except ValueError:\n                        cls_str = cls_raw.strip().lower()\n                        if cls_str in class_mapping:\n                            cls = class_mapping[cls_str]\n                        else:\n                            print(f\"Warning: Unknown class {cls_raw}, using default class 2\")\n                            cls = 2 \n                    \n                    annotations[frame_id].append({\n                        \"obj_id\": obj_id,\n                        \"bbox\": [x, y, w, h],\n                        \"class\": cls,\n                        \"visibility\": vis\n                    })\n                else:\n                    print(f\"Warning: Malformed annotation line: {line}\")\n        \n        return annotations\n    \n    def __getitem__(self, frame_id):\n        if self.annotations is None:\n            self.annotations = self._load_annotations()\n            \n        return self.annotations.get(frame_id, [])\n    \n    def keys(self):\n        if self.annotations is None:\n            self.annotations = self._load_annotations()\n            \n        return list(self.annotations.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:23:09.243040Z","iopub.execute_input":"2025-06-29T02:23:09.243302Z","iopub.status.idle":"2025-06-29T02:23:09.280580Z","shell.execute_reply.started":"2025-06-29T02:23:09.243283Z","shell.execute_reply":"2025-06-29T02:23:09.279848Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as T\nfrom PIL import Image\nclass LazyReIDPersonDataset(Dataset):\n    def __init__(self, video_paths, annotation_paths, transform=None):\n        self.video_paths = video_paths\n        self.annotation_paths = annotation_paths\n        self.transform = transform or T.Compose([\n            T.ToPILImage(),\n            T.Resize((256, 256)),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225])\n        ])\n        \n        self.index = []\n        \n        for video_path, annotation_path in zip(video_paths, annotation_paths):\n            frames_loader = LazyFrameLoader(video_path)\n            annotations_loader = LazyAnnotationLoader(annotation_path)\n            \n            for frame_id in annotations_loader.keys():\n                if frame_id in frames_loader.keys():\n                    annots = annotations_loader[frame_id]\n                    for ann in annots:\n                        obj_id = ann[\"obj_id\"]\n                        self.index.append((video_path, frame_id, ann[\"bbox\"], obj_id))\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, idx):\n        video_path, frame_id, bbox, obj_id = self.index[idx]\n        \n        img_files = sorted(os.listdir(video_path))\n        for file in img_files:\n            match = re.search(r'\\d+', file)\n            if match and int(match.group()) == frame_id:\n                img_path = os.path.join(video_path, file)\n                img = cv2.imread(img_path)\n                if img is not None:\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                    img_height, img_width = img.shape[:2]\n                    \n                    x, y, w, h = bbox\n                    \n                    x_clipped = max(0, int(x))\n                    y_clipped = max(0, int(y))\n                    x2_clipped = min(img_width, int(x + w))\n                    y2_clipped = min(img_height, int(y + h))\n                    \n                    w_clipped = x2_clipped - x_clipped\n                    h_clipped = y2_clipped - y_clipped\n                    \n                    if w_clipped > 0 and h_clipped > 0:\n                        cropped = img[y_clipped:y2_clipped, x_clipped:x2_clipped]\n                        \n                        if cropped.size > 0:\n                            img_tensor = self.transform(cropped)\n                            return img_tensor, obj_id\n        \n        print(\"can't load, so loading a blank\")\n        blank = np.zeros((128, 64, 3), dtype=np.uint8)\n        return self.transform(blank), obj_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:23:09.282378Z","iopub.execute_input":"2025-06-29T02:23:09.282595Z","iopub.status.idle":"2025-06-29T02:23:09.307086Z","shell.execute_reply.started":"2025-06-29T02:23:09.282561Z","shell.execute_reply":"2025-06-29T02:23:09.306581Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision.models as models\nimport torch.nn.functional as F\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclass ReIDModel(nn.Module):\n    def __init__(self, num_classes, dropout_rate=0.5, feature_dim=128, backbone='resnet18'):\n        super().__init__()\n        \n        if backbone == 'convnext_tiny':\n            convnext = models.convnext_tiny(weights=\"IMAGENET1K_V1\")\n            self.backbone = convnext.features \n            self.pool = nn.AdaptiveAvgPool2d((1, 1))\n            backbone_dim = 768  \n        elif backbone == 'resnet18':\n            resnet = models.resnet18(weights=\"IMAGENET1K_V1\")\n            self.backbone = nn.Sequential(*list(resnet.children())[:-2])\n            self.pool = nn.AdaptiveAvgPool2d((1, 1))\n            backbone_dim = 512 \n        else:\n            raise ValueError(f\"Unsupported backbone: {backbone}\")\n        \n        self.dropout = nn.Dropout(p=dropout_rate)\n        self.embedding = nn.Linear(backbone_dim, feature_dim)\n        self.dropout_feat = nn.Dropout(p=dropout_rate)\n        self.classifier = nn.Linear(feature_dim, num_classes)\n        \n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.pool(x).view(x.size(0), -1)\n        x = self.dropout(x)\n        feat = self.embedding(x)\n        feat = self.dropout_feat(feat)\n        out = self.classifier(feat)\n        return F.normalize(feat, dim=1), out\n# ##with hyperparameter tuning\n# class ReIDModel(nn.Module):\n#     def __init__(self, num_classes, backbone_name='resnet18', dropout_rate=0.5, feature_dim=128, input_dim=512):\n#         super().__init__()\n#         self.backbone, _ = get_backbone(backbone_name)  \n#         self.pool = nn.AdaptiveAvgPool2d((1, 1))\n#         self.dropout = nn.Dropout(p=dropout_rate)\n#         self.embedding = nn.Linear(input_dim, feature_dim)  \n#         self.dropout_feat = nn.Dropout(p=dropout_rate)\n#         self.classifier = nn.Linear(feature_dim, num_classes)\n    \n#     def forward(self, x):\n#         x = self.backbone(x)\n#         x = self.pool(x).view(x.size(0), -1)\n#         x = self.dropout(x)\n#         feat = self.embedding(x)\n#         feat = self.dropout_feat(feat)\n#         out = self.classifier(feat)\n#         return feat, out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:23:09.329405Z","iopub.execute_input":"2025-06-29T02:23:09.329662Z","iopub.status.idle":"2025-06-29T02:23:09.346503Z","shell.execute_reply.started":"2025-06-29T02:23:09.329640Z","shell.execute_reply":"2025-06-29T02:23:09.345787Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef split_dataset(dataset, train_ratio=0.7, val_ratio=0.15):\n    total = len(dataset)\n    indices = list(range(total))\n    train_idx, test_idx = train_test_split(indices, test_size=1-train_ratio)\n    val_relative = val_ratio / (1 - train_ratio)\n    val_idx, test_idx = train_test_split(test_idx, test_size=1 - val_relative)\n\n    from torch.utils.data import Subset\n    return Subset(dataset, train_idx), Subset(dataset, val_idx), Subset(dataset, test_idx)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:23:09.307786Z","iopub.execute_input":"2025-06-29T02:23:09.308017Z","iopub.status.idle":"2025-06-29T02:23:09.328535Z","shell.execute_reply.started":"2025-06-29T02:23:09.307993Z","shell.execute_reply":"2025-06-29T02:23:09.327816Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def train_reid(model, train_loader, val_loader, epochs=10, lr=1e-3, patience=5, \n               optimizer_type='adam', scheduler_type='plateau'):\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    if optimizer_type.lower() == 'adam':\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    elif optimizer_type.lower() == 'adamw':\n        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    elif optimizer_type.lower() == 'sgd':\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n    else:\n        raise ValueError(f\"Unsupported optimizer: {optimizer_type}\")\n    \n    if scheduler_type.lower() == 'plateau':\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=2)\n    elif scheduler_type.lower() == 'step':\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n    elif scheduler_type.lower() == 'cosine':\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    else:\n        raise ValueError(f\"Unsupported scheduler: {scheduler_type}\")\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    train_losses = []\n    train_accuracies = []\n    val_losses = []\n    val_accuracies = []\n    \n    best_val_acc = 0.0\n    no_improve_epochs = 0\n    best_model_state = None\n    \n    for epoch in range(epochs):\n        model.train()\n        total_loss, correct = 0, 0\n        batch_count = 0\n        \n        for imgs, labels in train_loader:\n            batch_count += 1\n            imgs, labels = imgs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            embeddings, logits = model(imgs)\n            loss = criterion(logits, labels)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            correct += (logits.argmax(1) == labels).sum().item()\n        \n        train_loss = total_loss / batch_count\n        train_acc = correct / len(train_loader.dataset)\n        \n        model.eval()\n        val_loss, val_correct = 0, 0\n        val_batch_count = 0\n        \n        with torch.no_grad():\n            for imgs, labels in val_loader:\n                val_batch_count += 1\n                imgs, labels = imgs.to(device), labels.to(device)\n                embeddings, logits = model(imgs)\n                loss = criterion(logits, labels)\n                val_loss += loss.item()\n                val_correct += (logits.argmax(1) == labels).sum().item()\n        \n        val_loss = val_loss / val_batch_count\n        val_acc = val_correct / len(val_loader.dataset)\n        \n        train_losses.append(train_loss)\n        train_accuracies.append(train_acc)\n        val_losses.append(val_loss)\n        val_accuracies.append(val_acc)\n        \n        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.3f}, Train Acc={train_acc:.3f}, \"\n              f\"Val Loss={val_loss:.3f}, Val Acc={val_acc:.3f}\")\n        \n        if scheduler_type.lower() == 'plateau':\n            scheduler.step(val_acc)\n        else:\n            scheduler.step()\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_model_state = model.state_dict().copy()\n            no_improve_epochs = 0\n            print(f\"New best validation accuracy: {best_val_acc:.3f}\")\n        else:\n            no_improve_epochs += 1\n            print(f\"No improvement for {no_improve_epochs} epochs\")\n            \n            if no_improve_epochs >= patience:\n                print(f\"Early stopping triggered after {epoch+1} epochs\")\n                break\n    \n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n        print(f\"Restored best model with validation accuracy: {best_val_acc:.3f}\")\n    \n    plot_training_metrics(train_losses, val_losses, train_accuracies, val_accuracies)\n    \n    return model\n\n\ndef plot_training_metrics(train_losses, val_losses, train_accuracies, val_accuracies):\n    \"\"\"\n    Plot training and validation metrics\n    \"\"\"\n    epochs_range = range(1, len(train_losses) + 1)\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    ax1.plot(epochs_range, train_losses, 'b-', label='Training Loss', linewidth=2)\n    ax1.plot(epochs_range, val_losses, 'r-', label='Validation Loss', linewidth=2)\n    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n    ax1.set_xlabel('Epochs', fontsize=12)\n    ax1.set_ylabel('Loss', fontsize=12)\n    ax1.legend(fontsize=10)\n    ax1.grid(True, alpha=0.3)\n    \n    ax2.plot(epochs_range, train_accuracies, 'b-', label='Training Accuracy', linewidth=2)\n    ax2.plot(epochs_range, val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\n    ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n    ax2.set_xlabel('Epochs', fontsize=12)\n    ax2.set_ylabel('Accuracy', fontsize=12)\n    ax2.legend(fontsize=10)\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nFinal Training Accuracy: {train_accuracies[-1]:.3f}\")\n    print(f\"Final Validation Accuracy: {val_accuracies[-1]:.3f}\")\n    print(f\"Best Validation Accuracy: {max(val_accuracies):.3f}\")\n\n# ##with hyperparameters tuning\n# def train_reid(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=10, patience=5):\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     model.to(device)\n\n#     best_val_acc = 0.0\n#     no_improve_epochs = 0\n#     best_model_state = None\n\n#     for epoch in range(epochs):\n#         model.train()\n#         total_loss, correct = 0, 0\n#         batch_count = 0\n#         for imgs, labels in train_loader:\n#             batch_count += 1\n#             imgs, labels = imgs.to(device), labels.to(device)\n#             optimizer.zero_grad()\n#             _, logits = model(imgs)\n#             loss = criterion(logits, labels)\n#             loss.backward()\n#             optimizer.step()\n#             total_loss += loss.item()\n#             correct += (logits.argmax(1) == labels).sum().item()\n            \n#         total_loss += loss.item()\n#         train_acc = correct / len(train_loader.dataset)\n\n#         model.eval()\n#         val_correct, val_loss = 0, 0\n#         val_batch_count = 0\n#         with torch.no_grad():\n#             for imgs, labels in val_loader:\n#                 val_batch_count += 1\n#                 imgs, labels = imgs.to(device), labels.to(device)\n#                 _, logits = model(imgs)\n#                 loss = criterion(logits, labels)\n#                 val_loss += loss.item()\n#                 val_correct += (logits.argmax(1) == labels).sum().item()\n\n#         val_acc = val_correct / len(val_loader.dataset)\n#         val_loss = val_loss / val_batch_count\n        \n#         print(f\"Epoch {epoch+1}: Train Loss={total_loss/batch_count:.3f}, Train Acc={train_acc:.3f}, \"\n#                f\"Val Loss={val_loss:.3f}, Val Acc={val_acc:.3f}\")\n        \n#         scheduler.step(val_acc)\n        \n#         if val_acc > best_val_acc:\n#             best_val_acc = val_acc\n#             best_model_state = deepcopy(model.state_dict())\n#             no_improve_epochs = 0\n#             print(f\"New best validation accuracy: {best_val_acc:.3f}\")\n#         else:\n#             no_improve_epochs += 1\n#             if no_improve_epochs >= patience:\n#                 break\n\n#     if best_model_state:\n#         model.load_state_dict(best_model_state)\n#         print(f\"Restored best model with validation accuracy: {best_val_acc:.3f}\")\n#     return model, best_val_acc\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:23:09.347214Z","iopub.execute_input":"2025-06-29T02:23:09.347479Z","iopub.status.idle":"2025-06-29T02:23:09.374160Z","shell.execute_reply.started":"2025-06-29T02:23:09.347448Z","shell.execute_reply":"2025-06-29T02:23:09.373658Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ndef evaluate_reid(model, test_loader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.eval()\n    y_true, y_pred = [], []\n    \n    with torch.no_grad():\n        for imgs, labels in test_loader:\n            imgs = imgs.to(device)\n            embeddings, logits = model(imgs) \n            y_true.extend(labels.tolist())\n            y_pred.extend(logits.argmax(1).cpu().tolist()) \n    \n    print(classification_report(y_true, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:23:09.374742Z","iopub.execute_input":"2025-06-29T02:23:09.374974Z","iopub.status.idle":"2025-06-29T02:23:09.394906Z","shell.execute_reply.started":"2025-06-29T02:23:09.374958Z","shell.execute_reply":"2025-06-29T02:23:09.394231Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\nclass LazyTrackingDataset(Dataset):\n    def __init__(self, frames_loader, annotations_loader, transform=None, object_transform=None, seq_length=5):\n        self.frames_loader = frames_loader\n        self.annotations_loader = annotations_loader\n        self.transform = transform\n        self.object_transform = object_transform\n        self.seq_length = seq_length\n        \n        self.frame_keys = sorted(frames_loader.keys())\n    \n    def __len__(self):\n        return max(0, len(self.frame_keys) - self.seq_length + 1)\n    \n    def __getitem__(self, index):\n        frame_ids = self.frame_keys[index: index + self.seq_length]\n        frames = []\n        \n        objects_across_sequence = {}\n        \n        for frame_id in frame_ids:\n            frame = self.frames_loader[frame_id]\n            \n            if self.transform:\n                transformed_frame = self.transform(frame)\n            else:\n                transformed_frame = torch.from_numpy(frame.transpose(2, 0, 1)).float() / 255.0\n                \n            frames.append(transformed_frame)\n            \n            frame_idx = frame_ids.index(frame_id)\n            for ann in self.annotations_loader[frame_id]:\n                obj_id = ann[\"obj_id\"]\n                \n                if obj_id not in objects_across_sequence:\n                    objects_across_sequence[obj_id] = {\n                        \"class\": ann[\"class\"],\n                        \"bboxes\": [None] * self.seq_length,\n                        \"appearances\": [None] * self.seq_length,\n                        \"valid_mask\": [0] * self.seq_length\n                    }\n                \n                objects_across_sequence[obj_id][\"bboxes\"][frame_idx] = ann[\"bbox\"]\n                objects_across_sequence[obj_id][\"valid_mask\"][frame_idx] = 1\n                \n                x, y, w, h = map(int, ann[\"bbox\"])\n                if x >= 0 and y >= 0 and w > 0 and h > 0 and x+w <= frame.shape[1] and y+h <= frame.shape[0]:\n                    crop = frame[y:y+h, x:x+w]\n                    \n                    if self.object_transform and crop.size > 0:\n                        crop_tensor = self.object_transform(crop)\n                    else:\n                        crop_tensor = torch.from_numpy(crop.transpose(2, 0, 1)).float() / 255.0\n                        \n                    objects_across_sequence[obj_id][\"appearances\"][frame_idx] = crop_tensor\n        \n        frame_sequence = torch.stack(frames)\n        \n        for obj_id in objects_across_sequence:\n            objects_across_sequence[obj_id][\"valid_mask\"] = torch.tensor(\n                objects_across_sequence[obj_id][\"valid_mask\"], dtype=torch.int64\n            )\n            \n            appearances = objects_across_sequence[obj_id][\"appearances\"]\n            valid_appearances = [a for a in appearances if a is not None]\n            \n            if valid_appearances:\n                template = valid_appearances[0]\n                \n                padded_appearances = []\n                for app in appearances:\n                    if app is not None:\n                        padded_appearances.append(app)\n                    else:\n                        padded_appearances.append(torch.zeros_like(template))\n                \n                objects_across_sequence[obj_id][\"appearances\"] = torch.stack(padded_appearances)\n            else:\n                default_shape = (3, 64, 128) \n                objects_across_sequence[obj_id][\"appearances\"] = torch.zeros(\n                    (self.seq_length, *default_shape), dtype=torch.float32\n                )\n            \n            padded_bboxes = []\n            for bbox in objects_across_sequence[obj_id][\"bboxes\"]:\n                if bbox is not None:\n                    padded_bboxes.append(torch.tensor(bbox, dtype=torch.float32))\n                else:\n                    padded_bboxes.append(torch.zeros(4, dtype=torch.float32))\n            \n            objects_across_sequence[obj_id][\"bboxes\"] = torch.stack(padded_bboxes)\n            \n        return {\n            \"frames\": frame_sequence,\n            \"objects\": objects_across_sequence\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:23:09.395716Z","iopub.execute_input":"2025-06-29T02:23:09.396328Z","iopub.status.idle":"2025-06-29T02:23:09.416917Z","shell.execute_reply.started":"2025-06-29T02:23:09.396307Z","shell.execute_reply":"2025-06-29T02:23:09.416271Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class RemappedDataset(Dataset):\n    def __init__(self, dataset, id_mapping):\n        self.dataset = dataset\n        self.id_mapping = id_mapping\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        img, orig_id = self.dataset[idx]\n        remapped_id = self.id_mapping[orig_id]\n        return img, remapped_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:23:09.447986Z","iopub.execute_input":"2025-06-29T02:23:09.448946Z","iopub.status.idle":"2025-06-29T02:23:09.469560Z","shell.execute_reply.started":"2025-06-29T02:23:09.448921Z","shell.execute_reply":"2025-06-29T02:23:09.468952Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### training with grid search","metadata":{}},{"cell_type":"code","source":"# from itertools import product\n# from torchvision import models\n# import torch.nn as nn\n# import torch.optim as optim\n\n# search_space = {\n#     #\"dropout_rate\": [0.0,0.1,0.2, 0.3, 0.5],\n#     #\"feature_dim\": [128, 256, 512],\n#     #\"backbone\": [\"resnet18\", \"resnet34\", \"efficientnet_v2_s\",\"mobilenet_v3_large\", \"convnext_tiny\",\"swin_t\", \"mobilenet_v3_small\",\"efficientnet_b0\", \"shufflenet_v2_x0_5\"  ],\n#     \"optimizer\": [\"adam\"],\n#     \"scheduler\": [\"step\", \"plateau\"],\n#     \"lr\": [2e-4, 3e-4,4e-4,  5e-4],\n# }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:23:09.470273Z","iopub.execute_input":"2025-06-29T02:23:09.470481Z","iopub.status.idle":"2025-06-29T02:23:09.485894Z","shell.execute_reply.started":"2025-06-29T02:23:09.470466Z","shell.execute_reply":"2025-06-29T02:23:09.485168Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# def get_backbone(name):\n#     if name == \"resnet18\":\n#         base = models.resnet18(weights=\"IMAGENET1K_V1\")\n#         input_dim = 512\n#     elif name == \"resnet34\":\n#         base = models.resnet34(weights=\"IMAGENET1K_V1\")\n#         input_dim = 512\n#     elif name == \"efficientnet_v2_s\":\n#         base = models.efficientnet_v2_s(weights=\"IMAGENET1K_V1\")\n#         input_dim = 1280\n#     elif name == \"mobilenet_v3_large\":\n#         base = models.mobilenet_v3_large(weights=\"IMAGENET1K_V1\")\n#         input_dim = 960\n#     elif name == \"convnext_tiny\":\n#         base = models.convnext_tiny(weights=\"IMAGENET1K_V1\")\n#         input_dim = 768\n#     elif name == \"swin_t\":\n#         base = models.swin_t(weights=\"IMAGENET1K_V1\")\n#         input_dim = 768\n#     elif name == \"mobilenet_v3_small\":\n#         base = models.mobilenet_v3_small(weights=\"IMAGENET1K_V1\")\n#         input_dim = 576\n#     elif name == \"efficientnet_b0\":\n#         base = models.efficientnet_b0(weights=\"IMAGENET1K_V1\")\n#         input_dim = 1280\n#     elif name == \"shufflenet_v2_x0_5\":\n#         base = models.shufflenet_v2_x0_5(weights=\"IMAGENET1K_V1\")\n#         input_dim = 1024\n#     else:\n#         raise ValueError(\"Unsupported backbone\")\n    \n#     backbone = nn.Sequential(*list(base.children())[:-2])\n#     return backbone, input_dim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:23:09.486635Z","iopub.execute_input":"2025-06-29T02:23:09.486788Z","iopub.status.idle":"2025-06-29T02:23:09.503078Z","shell.execute_reply.started":"2025-06-29T02:23:09.486776Z","shell.execute_reply":"2025-06-29T02:23:09.502414Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# def train_with_config(config):\n#     backbone, input_dim = get_backbone(config[\"backbone\"])\n    \n#     model = ReIDModel(\n#         num_classes=len(id2label),\n#         dropout_rate=config[\"dropout_rate\"],\n#         feature_dim=config[\"feature_dim\"],\n#         input_dim=input_dim  \n#     )\n#     model.backbone = backbone\n    \n#     \n#     criterion = nn.CrossEntropyLoss()\n    \n#     if config[\"optimizer\"] == \"adam\":\n#         optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n#     elif config[\"optimizer\"] == \"sgd\":\n#         optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9)\n    \n#     if config[\"scheduler\"] == \"plateau\":\n#         scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n#     elif config[\"scheduler\"] == \"step\":\n#         scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n    \n#     model, best_val_acc = train_reid(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=100, patience=10)\n#     return model, best_val_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:23:09.503817Z","iopub.execute_input":"2025-06-29T02:23:09.504204Z","iopub.status.idle":"2025-06-29T02:23:09.524094Z","shell.execute_reply.started":"2025-06-29T02:23:09.504171Z","shell.execute_reply":"2025-06-29T02:23:09.523616Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# lazy_datasets, video_paths, annotation_paths = load_lazy_dataset_from_directory(\n#     \"/kaggle/input/aquagaurd-drowning-tracking-dataset/Drowning Tracking Dataset\", \n#     seq_length=25\n# )\n\n# reid_dataset = LazyReIDPersonDataset(video_paths, annotation_paths)\n\n# unique_ids = set()\n# for _, obj_id in reid_dataset:\n#     unique_ids.add(obj_id)\n# unique_ids = sorted(unique_ids)\n# id2label = {orig: idx for idx, orig in enumerate(unique_ids)}\n\n# remapped_dataset = RemappedDataset(reid_dataset, id2label)\n\n# indices = list(range(len(remapped_dataset)))\n# train_indices, val_test_indices = train_test_split(indices, test_size=0.4, random_state=42)\n# val_indices, test_indices = train_test_split(val_test_indices, test_size=0.5, random_state=42)\n\n# train_data = Subset(remapped_dataset, train_indices)\n# val_data = Subset(remapped_dataset, val_indices)\n# test_data = Subset(remapped_dataset, test_indices)\n\n# batch_size=32\n# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n# val_loader = DataLoader(val_data, batch_size=batch_size, num_workers=4)\n# test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:23:09.524762Z","iopub.execute_input":"2025-06-29T02:23:09.524919Z","iopub.status.idle":"2025-06-29T02:23:09.544460Z","shell.execute_reply.started":"2025-06-29T02:23:09.524907Z","shell.execute_reply":"2025-06-29T02:23:09.543802Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# from itertools import product\n# from copy import deepcopy\n# best_acc = 0\n# best_config = None\n# best_model = None\n\n# for combo in product(*search_space.values()):\n#     keys = list(search_space.keys())\n#     config = dict(zip(keys, combo))\n#     print(f\"\\n🚀 Running config: {config}\")\n    \n#     model, val_acc = train_with_config(config)\n#     print(f\"✅ Validation Accuracy: {val_acc:.4f}\")\n    \n#     if val_acc > best_acc:\n#         best_acc = val_acc\n#         best_config = config\n#         best_model = model\n#         print(f\"🔥 New best config found with acc: {best_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:23:09.545117Z","iopub.execute_input":"2025-06-29T02:23:09.545333Z","iopub.status.idle":"2025-06-29T02:23:09.565641Z","shell.execute_reply.started":"2025-06-29T02:23:09.545318Z","shell.execute_reply":"2025-06-29T02:23:09.565049Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## without grid search","metadata":{}},{"cell_type":"code","source":"\nlazy_datasets, video_paths, annotation_paths = load_lazy_dataset_from_directory(\n    \"/kaggle/input/aquagaurd-drowning-tracking-dataset/Drowning Tracking Dataset\", \n    seq_length=25\n)\n\nreid_dataset = LazyReIDPersonDataset(video_paths, annotation_paths)\n\nunique_ids = set()\nfor _, obj_id in reid_dataset:\n    unique_ids.add(obj_id)\nunique_ids = sorted(unique_ids)\nid2label = {orig: idx for idx, orig in enumerate(unique_ids)}\n\nremapped_dataset = RemappedDataset(reid_dataset, id2label)\n\nindices = list(range(len(remapped_dataset)))\ntrain_indices, val_test_indices = train_test_split(indices, test_size=0.4, random_state=42)\nval_indices, test_indices = train_test_split(val_test_indices, test_size=0.5, random_state=42)\n\ntrain_data = Subset(remapped_dataset, train_indices)\nval_data = Subset(remapped_dataset, val_indices)\ntest_data = Subset(remapped_dataset, test_indices)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_data, batch_size=batch_size, num_workers=4)\ntest_loader = DataLoader(test_data, batch_size=batch_size, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:23:09.566293Z","iopub.execute_input":"2025-06-29T02:23:09.566501Z","iopub.status.idle":"2025-06-29T02:26:18.688001Z","shell.execute_reply.started":"2025-06-29T02:23:09.566481Z","shell.execute_reply":"2025-06-29T02:26:18.687428Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"config = {\n    'dropout_rate': 0.1, \n    'feature_dim': 512, \n    'backbone': 'convnext_tiny', \n    'optimizer': 'adam', \n    'scheduler': 'plateau', \n    'lr': 0.0002\n}\nnum_epochs = 100\n\nmodel = ReIDModel(\n    num_classes=len(unique_ids),\n    dropout_rate=config['dropout_rate'],\n    feature_dim=config['feature_dim'],\n    backbone=config['backbone']\n)\n\ntrained_model = train_reid(\n    model, \n    train_loader, \n    val_loader, \n    epochs=num_epochs, \n    lr=config['lr'], \n    patience=10,\n    optimizer_type=config['optimizer'],\n    scheduler_type=config['scheduler']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T02:26:18.688792Z","iopub.execute_input":"2025-06-29T02:26:18.689059Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n100%|██████████| 109M/109M [00:00<00:00, 190MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=2.764, Train Acc=0.278, Val Loss=2.333, Val Acc=0.341\nNew best validation accuracy: 0.341\nEpoch 2: Train Loss=1.984, Train Acc=0.425, Val Loss=1.761, Val Acc=0.446\nNew best validation accuracy: 0.446\nEpoch 3: Train Loss=1.284, Train Acc=0.613, Val Loss=1.224, Val Acc=0.629\nNew best validation accuracy: 0.629\nEpoch 4: Train Loss=0.820, Train Acc=0.749, Val Loss=0.990, Val Acc=0.709\nNew best validation accuracy: 0.709\nEpoch 5: Train Loss=0.534, Train Acc=0.835, Val Loss=0.948, Val Acc=0.737\nNew best validation accuracy: 0.737\nEpoch 6: Train Loss=0.358, Train Acc=0.888, Val Loss=0.892, Val Acc=0.761\nNew best validation accuracy: 0.761\nEpoch 7: Train Loss=0.258, Train Acc=0.920, Val Loss=0.980, Val Acc=0.754\nNo improvement for 1 epochs\nEpoch 8: Train Loss=0.204, Train Acc=0.939, Val Loss=0.921, Val Acc=0.779\nNew best validation accuracy: 0.779\n","output_type":"stream"}],"execution_count":null}]}